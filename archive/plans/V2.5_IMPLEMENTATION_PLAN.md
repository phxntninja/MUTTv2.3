# MUTT v2.5 Enterprise Readiness Implementation Plan

**Generated:** 2025-11-09
**Status:** Ready to Execute
**Target:** Transform v2.3 into Fortune 500-ready v2.5

---

## Executive Summary

This plan implements 10 enterprise-grade enhancements to MUTT v2.3, broken into **60+ atomic tasks** organized into **5 phases**. Each task is sized for 15-45 minutes of focused work and can be paused/resumed without issues.

**Total Estimated Time:** 25-35 hours
**Recommended Schedule:** 2-3 weeks of part-time development

---

## Implementation Philosophy

### Task Sizing Principles
- ✅ Each task creates ONE testable artifact
- ✅ Tasks are atomic (complete or not started, no partial state)
- ✅ Maximum 45 minutes per task
- ✅ Clear acceptance criteria for each task
- ✅ Can be paused between any two tasks

### Dependency Management
- Tasks marked with `[Depends: X.Y]` require completion of task X.Y first
- Tasks within same section can generally run in parallel
- Infrastructure tasks (Phase 1) must complete before feature tasks

---

## Phase 1: Infrastructure & Database (Foundation)

**Goal:** Add database tables and core utilities needed by all features
**Estimated Time:** 4-6 hours
**Tasks:** 8

### 1.1 Config Audit Infrastructure

#### Task 1.1.1: Create Config Audit Schema
**File:** `database/config_audit_schema.sql`
**Time:** 20 min
**Acceptance Criteria:**
- Table `config_audit_log` created with all fields from handoff doc
- 2 indexes created (`table_record`, `changed_at`)
- File includes sample INSERT for testing

**Implementation:**
```sql
CREATE TABLE config_audit_log (
    id BIGSERIAL PRIMARY KEY,
    changed_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    changed_by VARCHAR(100) NOT NULL,
    operation VARCHAR(10) NOT NULL CHECK (operation IN ('CREATE', 'UPDATE', 'DELETE')),
    table_name VARCHAR(50) NOT NULL,
    record_id INTEGER NOT NULL,
    old_values JSONB,
    new_values JSONB,
    reason TEXT,
    correlation_id VARCHAR(36)
);

CREATE INDEX idx_config_audit_log_table_record ON config_audit_log(table_name, record_id);
CREATE INDEX idx_config_audit_log_changed_at ON config_audit_log(changed_at DESC);
```

#### Task 1.1.2: Update postgres-init.sql
**File:** `database/postgres-init.sql`
**Time:** 10 min
**Acceptance Criteria:**
- Adds source command to load `config_audit_schema.sql`
- Maintains backward compatibility with existing schema

#### Task 1.1.3: Create Audit Helper Library
**File:** `services/audit_logger.py`
**Time:** 30 min
**Acceptance Criteria:**
- Function `log_config_change(conn, changed_by, operation, table_name, record_id, old_values, new_values, reason=None)`
- Uses parameterized queries (SQL injection safe)
- Returns audit log ID
- Includes 3 unit tests

**Implementation Sketch:**
```python
import json
from datetime import datetime

def log_config_change(conn, changed_by, operation, table_name, record_id,
                     old_values, new_values, reason=None, correlation_id=None):
    """
    Log a configuration change to audit trail.

    Args:
        conn: psycopg2 connection
        changed_by: Username or API key name
        operation: 'CREATE', 'UPDATE', or 'DELETE'
        table_name: Which table was modified
        record_id: ID of the record
        old_values: dict of old values (None for CREATE)
        new_values: dict of new values (None for DELETE)
        reason: Optional reason for change
        correlation_id: Optional correlation ID for request tracking

    Returns:
        int: audit_log_id
    """
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO config_audit_log
        (changed_by, operation, table_name, record_id, old_values, new_values, reason, correlation_id)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        RETURNING id
    """, (
        changed_by,
        operation,
        table_name,
        record_id,
        json.dumps(old_values) if old_values else None,
        json.dumps(new_values) if new_values else None,
        reason,
        correlation_id
    ))
    audit_id = cursor.fetchone()[0]
    conn.commit()
    return audit_id
```

---

### 1.2 Data Retention Infrastructure

#### Task 1.2.1: Create Partitioned Event Audit Log Schema
**File:** `database/partitioned_event_audit_log.sql`
**Time:** 30 min
**Acceptance Criteria:**
- Partitioned table `event_audit_log_v2` created (by month)
- Includes migration script from old `event_audit_log`
- Creates partitions for next 3 months
- Archive table `event_audit_log_archive` created

#### Task 1.2.2: Create Partition Manager Script
**File:** `scripts/create_monthly_partitions.py`
**Time:** 35 min
**Acceptance Criteria:**
- Python script that creates partitions for next N months
- Can be run monthly via cron
- Includes dry-run mode
- Logs all actions

#### Task 1.2.3: Create Archive Script
**File:** `scripts/archive_old_events.py`
**Time:** 40 min
**Acceptance Criteria:**
- Moves events older than `RETENTION_DAYS` to archive
- Deletes from main table after archival
- Configurable via env var `EVENT_RETENTION_DAYS`
- Includes rollback on error
- Logs row counts

---

### 1.3 Dynamic Config Infrastructure

#### Task 1.3.1: Create DynamicConfig Class
**File:** `services/dynamic_config.py`
**Time:** 45 min
**Acceptance Criteria:**
- Class `DynamicConfig` with methods: `get()`, `set()`, `load_all()`
- 5-second local cache with TTL
- Redis backend for storage
- Includes 5 unit tests

**Implementation Sketch:**
```python
import time
import logging

logger = logging.getLogger(__name__)

class DynamicConfig:
    """
    Dynamic configuration with Redis backend and local caching.

    Allows runtime configuration changes without service restart.
    """

    def __init__(self, redis_client, prefix="mutt:config"):
        self.redis = redis_client
        self.prefix = prefix
        self.cache = {}
        self.load_all()

    def get(self, key, default=None):
        """Get config value with 5s local cache."""
        # Check local cache first
        if key in self.cache and time.time() - self.cache[key]['ts'] < 5:
            return self.cache[key]['value']

        # Fetch from Redis
        redis_key = f"{self.prefix}:{key}"
        value = self.redis.get(redis_key)

        if value is not None:
            value = value.decode('utf-8') if isinstance(value, bytes) else value
            self.cache[key] = {'value': value, 'ts': time.time()}
            logger.debug(f"Config loaded from Redis: {key}={value}")
            return value

        if default is not None:
            return default

        raise KeyError(f"Config key not found: {key}")

    def set(self, key, value):
        """Set config value in Redis and update cache."""
        redis_key = f"{self.prefix}:{key}"
        self.redis.set(redis_key, str(value))
        self.cache[key] = {'value': str(value), 'ts': time.time()}
        logger.info(f"Config updated: {key}={value}")

        # Publish change notification
        self.redis.publish(f"{self.prefix}:updates", key)

    def load_all(self):
        """Load all config from Redis on startup."""
        count = 0
        for redis_key in self.redis.scan_iter(f"{self.prefix}:*"):
            key_name = redis_key.decode('utf-8').split(':')[-1]
            if key_name == 'updates':  # Skip pubsub channel
                continue
            value = self.redis.get(redis_key)
            if value:
                self.cache[key_name] = {
                    'value': value.decode('utf-8') if isinstance(value, bytes) else value,
                    'ts': time.time()
                }
                count += 1
        logger.info(f"Loaded {count} config values from Redis")
```

#### Task 1.3.2: Create Config Watcher Thread
**File:** Update `services/dynamic_config.py`
**Time:** 30 min
**Depends:** Task 1.3.1
**Acceptance Criteria:**
- Background thread that subscribes to `mutt:config:updates`
- Invalidates local cache when config changes
- Graceful shutdown on SIGTERM
- Includes 2 unit tests for cache invalidation

---

## Phase 2: Core Features - Hot Reload & Secrets (High Priority)

**Goal:** Enable zero-downtime operations
**Estimated Time:** 6-8 hours
**Tasks:** 10

### 2.1 Configuration Hot-Reloading

#### Task 2.1.1: Initialize Default Configs in Redis
**File:** `scripts/init_dynamic_config.py`
**Time:** 25 min
**Depends:** Task 1.3.1
**Acceptance Criteria:**
- Script loads env vars into Redis on first deploy
- Maps all current env vars to Redis keys
- Includes dry-run mode
- Can be run as Kubernetes init container

#### Task 2.1.2: Integrate DynamicConfig into Ingestor Service
**File:** `services/ingestor_service.py`
**Time:** 35 min
**Depends:** Task 1.3.1, Task 2.1.1
**Acceptance Criteria:**
- Replace `CACHE_RELOAD_INTERVAL` env var with `config.get('cache_reload_interval')`
- Replace `MAX_INGEST_QUEUE_SIZE` with dynamic config
- Service can change these at runtime
- Add `/admin/config` endpoint to view current config

#### Task 2.1.3: Integrate DynamicConfig into Alerter Service
**File:** `services/alerter_service.py`
**Time:** 35 min
**Depends:** Task 1.3.1, Task 2.1.1
**Acceptance Criteria:**
- Replace `CACHE_RELOAD_INTERVAL` with dynamic config
- Service can change at runtime
- Config changes logged to audit trail

#### Task 2.1.4: Integrate DynamicConfig into Moog Forwarder
**File:** `services/moog_forwarder_service.py`
**Time:** 35 min
**Depends:** Task 1.3.1, Task 2.1.1
**Acceptance Criteria:**
- Replace `MOOG_RATE_LIMIT` with dynamic config
- Replace `MOOG_BATCH_SIZE` with dynamic config
- Service can change rate limits at runtime without restart

#### Task 2.1.5: Create Config Management API Endpoints
**File:** `services/web_ui_service.py`
**Time:** 40 min
**Depends:** Task 1.3.1, Task 1.1.3
**Acceptance Criteria:**
- `GET /api/v1/config` - List all dynamic configs
- `PUT /api/v1/config/<key>` - Update config value
- `GET /api/v1/config/history` - Show recent config changes
- All changes logged to `config_audit_log`

#### Task 2.1.6: Add Config Change Unit Tests
**File:** `tests/test_dynamic_config.py`
**Time:** 30 min
**Depends:** Task 1.3.1
**Acceptance Criteria:**
- 8+ tests covering get/set/cache TTL
- Test cache invalidation on update
- Test Redis connection failure handling

---

### 2.2 Zero-Downtime Secret Rotation

#### Task 2.2.1: Update Vault Secret Structure
**File:** `scripts/vault-init.sh`
**Time:** 20 min
**Acceptance Criteria:**
- Adds `_CURRENT` and `_NEXT` suffixes to all secrets
- Sets `_ROTATION_TIME` for each rotatable secret
- Backward compatible with existing secrets

#### Task 2.2.2: Create Dual-Password Redis Connection Helper
**File:** `services/redis_connector.py`
**Time:** 35 min
**Acceptance Criteria:**
- Function `get_redis_connection(config, secrets)` tries CURRENT then NEXT
- Logs which password succeeded
- Falls back gracefully
- Includes 4 unit tests

#### Task 2.2.3: Create Dual-Password PostgreSQL Connection Helper
**File:** `services/postgres_connector.py`
**Time:** 35 min
**Acceptance Criteria:**
- Function `get_postgres_connection(config, secrets)` tries CURRENT then NEXT
- Connection pooling support
- Includes 4 unit tests

#### Task 2.2.4: Integrate Dual-Password into All Services
**File:** All 4 service files
**Time:** 40 min
**Depends:** Task 2.2.2, Task 2.2.3
**Acceptance Criteria:**
- All services use new connection helpers
- Services can survive password rotation without restart
- Tested with mock password changes

#### Task 2.2.5: Create Secret Rotation Procedure Doc
**File:** `docs/SECRET_ROTATION_PROCEDURE.md`
**Time:** 25 min
**Acceptance Criteria:**
- Step-by-step rotation procedure
- Rollback instructions
- Verification checklist

---

## Phase 3: Reliability & Observability (Medium Priority)

**Goal:** Production-grade reliability and monitoring
**Estimated Time:** 7-9 hours
**Tasks:** 12

### 3.1 Advanced Backpressure & Load Shedding

#### Task 3.1.1: Add Global Rate Limiter to Ingestor
**File:** `services/ingestor_service.py`
**Time:** 30 min
**Acceptance Criteria:**
- Redis-based global rate limit (across all pods)
- Returns 429 when limit exceeded
- Metric `mutt_ingest_rate_limited_total`
- Configurable via `INGEST_MAX_RATE` (dynamic config)

#### Task 3.1.2: Add Queue Depth Backpressure to Alerter
**File:** `services/alerter_service.py`
**Time:** 30 min
**Acceptance Criteria:**
- Checks queue depth before processing
- Pauses processing if depth > `MAX_QUEUE_DEPTH`
- Metric `mutt_alerter_backpressure_active` (gauge)
- Logs backpressure events

#### Task 3.1.3: Add Circuit Breaker State Metrics
**File:** `services/moog_forwarder_service.py`
**Time:** 25 min
**Acceptance Criteria:**
- Metric `mutt_moog_circuit_breaker_state` (gauge: 0=closed, 1=open)
- Metric `mutt_moog_circuit_breaker_trips_total` (counter)
- Logs circuit breaker state changes

---

### 3.2 Self-Healing & Auto-Remediation

#### Task 3.2.1: Create Auto-Remediation Service
**File:** `services/remediation_service.py`
**Time:** 45 min
**Acceptance Criteria:**
- Checks Moogsoft health endpoint
- Replays DLQ messages if Moogsoft healthy
- Alerts on poison messages in alerter DLQ
- Runs every 5 minutes (configurable)

#### Task 3.2.2: Add Moogsoft Health Check Function
**File:** `services/remediation_service.py`
**Time:** 20 min
**Acceptance Criteria:**
- Sends test request to Moogsoft API
- Returns True/False for health status
- Cached result (60s TTL)

#### Task 3.2.3: Create Remediation Metrics
**File:** `services/remediation_service.py`
**Time:** 15 min
**Acceptance Criteria:**
- Metric `mutt_auto_remediation_total{action="dlq_replay"}` (counter)
- Metric `mutt_moog_health_status` (gauge)

#### Task 3.2.4: Add Remediation to Docker Compose
**File:** `docker-compose.yml`
**Time:** 15 min
**Acceptance Criteria:**
- Service `mutt-remediation` defined
- Runs every 5 minutes
- Depends on Redis and all other services

#### Task 3.2.5: Create Remediation Unit Tests
**File:** `tests/test_remediation.py`
**Time:** 35 min
**Acceptance Criteria:**
- 8+ tests covering DLQ replay logic
- Test health check caching
- Test metric increments

---

### 3.3 SLO Tracking & Compliance

#### Task 3.3.1: Define SLO Targets
**File:** `services/slo_definitions.py`
**Time:** 30 min
**Acceptance Criteria:**
- Dict defining 5+ SLOs with targets
- P95/P99 latency targets
- Success rate targets
- Error budget calculations

#### Task 3.3.2: Create SLO Compliance Checker
**File:** `services/slo_checker.py`
**Time:** 40 min
**Depends:** Task 3.3.1
**Acceptance Criteria:**
- Queries Prometheus for actual metrics
- Compares against SLO targets
- Returns compliance report (JSON)
- Can be run on-demand or scheduled

#### Task 3.3.3: Add SLO Dashboard Endpoint
**File:** `services/web_ui_service.py`
**Time:** 30 min
**Depends:** Task 3.3.2
**Acceptance Criteria:**
- `GET /api/v1/slo/status` returns current SLO compliance
- Shows error budget remaining
- Color-coded status (green/yellow/red)

#### Task 3.3.4: Create Prometheus SLO Recording Rules
**File:** `configs/prometheus/slo_rules.yml`
**Time:** 35 min
**Depends:** Task 3.3.1
**Acceptance Criteria:**
- Recording rules for SLO metrics
- 5m and 1h burn rate calculations
- Update `prometheus.yml` to load new rules

---

## Phase 4: API & Compliance (Enterprise Requirements)

**Goal:** Enterprise API standards and audit compliance
**Estimated Time:** 5-7 hours
**Tasks:** 10

### 4.1 Configuration Change Audit

#### Task 4.1.1: Integrate Audit Logging into Rule CRUD
**File:** `services/web_ui_service.py`
**Time:** 40 min
**Depends:** Task 1.1.3
**Acceptance Criteria:**
- All rule CREATE/UPDATE/DELETE operations logged
- Captures old and new values
- Includes `changed_by` from API key or session
- Includes optional `reason` field

#### Task 4.1.2: Add Audit Log API Endpoints
**File:** `services/web_ui_service.py`
**Time:** 35 min
**Depends:** Task 1.1.1, Task 4.1.1
**Acceptance Criteria:**
- `GET /api/v1/audit/config?table_name=alert_rules&record_id=42`
- Returns audit history for specific record
- Includes pagination
- Sortable by timestamp

#### Task 4.1.3: Create Audit Log UI Component
**File:** `services/web_ui_service.py` (HTML template)
**Time:** 30 min
**Depends:** Task 4.1.2
**Acceptance Criteria:**
- Dashboard shows recent config changes
- Click rule to see its audit history
- Shows diff of old vs new values

---

### 4.2 API Versioning & Deprecation

#### Task 4.2.1: Add Version Headers to All Responses
**File:** `services/web_ui_service.py`
**Time:** 25 min
**Acceptance Criteria:**
- All API responses include `X-API-Version: v2.5`
- Deprecated endpoints include `X-Sunset` header
- Add to API documentation

#### Task 4.2.2: Create Versioned Endpoint Decorator
**File:** `services/api_versioning.py`
**Time:** 35 min
**Acceptance Criteria:**
- Decorator `@versioned_endpoint(min_version='v2', max_version='v3')`
- Returns 400 if client requests unsupported version
- Client specifies version via header `Accept-Version: v2`

#### Task 4.2.3: Implement v1 Legacy Compatibility
**File:** `services/web_ui_service.py`
**Time:** 40 min
**Depends:** Task 4.2.2
**Acceptance Criteria:**
- `/api/v1/rules` returns old format (array)
- `/api/v2/rules` returns new format (object with metadata)
- Both endpoints functional
- Include deprecation notice in v1 response

#### Task 4.2.4: Update API Documentation
**File:** `docs/API_REFERENCE.md`
**Time:** 30 min
**Acceptance Criteria:**
- Documents versioning strategy
- Lists all v1, v2, v2.5 endpoints
- Deprecation timeline
- Migration guide for v1→v2

---

### 4.3 Data Retention Compliance

#### Task 4.3.1: Add Retention Policy Config
**File:** `.env.template`
**Time:** 10 min
**Acceptance Criteria:**
- `EVENT_RETENTION_DAYS=90`
- `ARCHIVE_RETENTION_DAYS=2555` (7 years for compliance)
- `PARTITION_ADVANCE_MONTHS=3`

#### Task 4.3.2: Create Retention Policy Enforcement
**File:** `scripts/enforce_retention.py`
**Time:** 35 min
**Depends:** Task 1.2.3, Task 4.3.1
**Acceptance Criteria:**
- Archives events older than retention period
- Deletes archived events older than archive retention
- Dry-run mode
- Logs all deletions

#### Task 4.3.3: Add Retention to Kubernetes CronJob
**File:** `k8s/cronjob-retention.yaml`
**Time:** 20 min
**Depends:** Task 4.3.2
**Acceptance Criteria:**
- Runs daily at 2 AM
- Logs to persistent volume
- Alerts on failure

#### Task 4.3.4: Create Retention Metrics
**File:** `scripts/enforce_retention.py`
**Time:** 15 min
**Acceptance Criteria:**
- Metric `mutt_events_archived_total` (counter)
- Metric `mutt_events_deleted_total` (counter)
- Metric `mutt_oldest_event_age_days` (gauge)

---

## Phase 5: Developer Experience & Documentation

**Goal:** Streamline onboarding and maintain decisions
**Estimated Time:** 4-6 hours
**Tasks:** 10

### 5.1 Developer CLI (muttdev)

#### Task 5.1.1: Create muttdev CLI Structure
**File:** `tools/muttdev/cli.py`
**Time:** 30 min
**Acceptance Criteria:**
- Click-based CLI framework
- Commands: `setup`, `config`, `test`, `logs`
- `--help` works for all commands

#### Task 5.1.2: Implement `muttdev setup`
**File:** `tools/muttdev/commands/setup.py`
**Time:** 40 min
**Acceptance Criteria:**
- Creates `.env` from template
- Runs `docker-compose up -d`
- Waits for services to be healthy
- Loads sample data
- Success message with URLs

#### Task 5.1.3: Implement `muttdev config`
**File:** `tools/muttdev/commands/config.py`
**Time:** 35 min
**Depends:** Task 1.3.1
**Acceptance Criteria:**
- `muttdev config get cache_reload_interval`
- `muttdev config set cache_reload_interval 600`
- `muttdev config list` shows all configs
- Connects to local Redis

#### Task 5.1.4: Implement `muttdev logs`
**File:** `tools/muttdev/commands/logs.py`
**Time:** 25 min
**Acceptance Criteria:**
- `muttdev logs <service>` streams logs
- `muttdev logs --tail=100 ingestor`
- `muttdev logs --follow alerter`

#### Task 5.1.5: Create muttdev Installation Script
**File:** `tools/install.sh`
**Time:** 20 min
**Acceptance Criteria:**
- One-command install: `curl ... | bash`
- Installs Python dependencies
- Adds `muttdev` to PATH
- Verifies installation

---

### 5.2 Architecture Decision Records

#### Task 5.2.1: Create ADR Template
**File:** `docs/adrs/template.md`
**Time:** 15 min
**Acceptance Criteria:**
- Sections: Status, Context, Decision, Consequences
- Example ADR included

#### Task 5.2.2: Write ADR-001: Why Redis over Kafka
**File:** `docs/adrs/ADR-001-redis-over-kafka.md`
**Time:** 30 min
**Acceptance Criteria:**
- Explains decision context
- Lists pros/cons
- Includes performance data

#### Task 5.2.3: Write ADR-002: Why Vault over K8s Secrets
**File:** `docs/adrs/ADR-002-vault-over-k8s-secrets.md`
**Time:** 25 min
**Acceptance Criteria:**
- Security rationale
- Rotation capabilities
- Audit trail

#### Task 5.2.4: Write ADR-003: Single-Threaded Workers
**File:** `docs/adrs/ADR-003-single-threaded-workers.md`
**Time:** 25 min
**Acceptance Criteria:**
- Explains BRPOPLPUSH requirement
- GIL considerations
- Horizontal scaling approach

#### Task 5.2.5: Write ADR-004: PostgreSQL for Audit Logs
**File:** `docs/adrs/ADR-004-postgresql-audit-logs.md`
**Time:** 25 min
**Acceptance Criteria:**
- ACID requirements
- Compliance considerations
- JSONB for flexibility

---

### 5.3 Final Documentation & Testing

#### Task 5.3.1: Update README for v2.5
**File:** `README.md`
**Time:** 30 min
**Acceptance Criteria:**
- Add v2.5 features section
- Update architecture diagram
- Link to new docs

#### Task 5.3.2: Create v2.5 Migration Guide
**File:** `docs/MIGRATION_v2.3_to_v2.5.md`
**Time:** 35 min
**Acceptance Criteria:**
- Step-by-step upgrade procedure
- Database migration steps
- Config changes required
- Rollback instructions

#### Task 5.3.3: Create v2.5 Feature Comparison Matrix
**File:** `docs/FEATURE_MATRIX.md`
**Time:** 20 min
**Acceptance Criteria:**
- Table comparing v2.3 vs v2.5
- Business value for each feature
- Links to relevant docs

#### Task 5.3.4: Integration Test Suite for v2.5 Features
**File:** `tests/test_integration_v25.py`
**Time:** 45 min
**Acceptance Criteria:**
- Tests config hot-reload end-to-end
- Tests secret rotation
- Tests auto-remediation
- Tests audit logging

---

## Testing & Validation Checklist

### Per-Task Testing
- [ ] Unit tests pass for new code
- [ ] Linting passes (flake8, black)
- [ ] No secrets committed

### Per-Phase Testing
- [ ] Phase integration tests pass
- [ ] Docker Compose still works
- [ ] Existing v2.3 functionality unaffected
- [ ] Documentation updated

### Final Validation (After All Phases)
- [ ] All 135+ existing tests pass
- [ ] New tests added for v2.5 features (target: 40+ new tests)
- [ ] Coverage remains above 80%
- [ ] CI/CD pipeline passes
- [ ] Docker build succeeds
- [ ] Kubernetes manifests validated
- [ ] Load testing shows no regression
- [ ] Security scan passes (Bandit, Safety)

---

## Rollout Strategy

### Development Environment (Week 1)
1. Complete Phase 1 (Infrastructure)
2. Test in local Docker Compose
3. Validate database migrations

### Staging Environment (Week 2)
1. Complete Phases 2-3 (Core Features + Reliability)
2. Load testing
3. Chaos engineering tests

### Production Rollout (Week 3)
1. Complete Phases 4-5 (Compliance + DevEx)
2. Gradual rollout (10% → 50% → 100% traffic)
3. Monitor SLO compliance

---

## Risk Mitigation

### High-Risk Areas
1. **Database Migrations** - Test on production snapshot first
2. **Secret Rotation** - Have rollback plan ready
3. **Config Hot-Reload** - Validate all services can handle config changes

### Rollback Plan
- Each phase creates a git tag: `v2.5-phase1`, `v2.5-phase2`, etc.
- Database migrations include rollback scripts
- Feature flags for new functionality

---

## Success Metrics

### Technical Metrics
- Zero-downtime config changes: 100% success rate
- Secret rotation without restart: 100% success rate
- SLO compliance: 99.9%+
- Test coverage: >80%

### Business Metrics
- Time to onboard new developer: <30 minutes
- Config change audit trail: 100% coverage
- Data retention compliance: Automated

---

## Next Steps

**To start implementation:**

```bash
cd "C:\Users\ronhi\OneDrive\Documents\AI Work\MUTT-rewrites\Kimi re-write"

# Start with Phase 1, Task 1.1.1
echo "Ready to implement Task 1.1.1: Create Config Audit Schema"
```

**Recommended workflow:**
1. Create feature branch: `git checkout -b feature/v2.5-phase1`
2. Complete tasks sequentially within each phase
3. Commit after each task completion
4. Test at end of each phase
5. Merge to main after phase validation

---

## Questions or Blockers?

- For task clarification, reference the handoff document
- For architecture decisions, create an ADR
- For testing help, see `tests/README_TESTS.md`
- For deployment issues, see `QUICKSTART.md`

**Total Tasks:** 60+
**Estimated Completion:** 25-35 hours
**Ready to Start:** Yes ✅
