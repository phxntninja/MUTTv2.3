# MUTT Scalability Guide

**Version:** 1.0
**Last Updated:** 2025-11-10
**Status:** Draft
**Audience:** Architects, Operators, SREs
**Prerequisites:** `SYSTEM_ARCHITECTURE.md`

---

## Table of Contents
1. [Scalability Model](#scalability-model)
2. [Capacity Planning](#capacity-planning)
3. [Performance Characteristics](#performance-characteristics)
4. [Scaling Strategies](#scaling-strategies)
5. [Bottleneck Identification](#bottleneck-identification)
6. [Optimization Strategies](#optimization-strategies)

---

## Scalability Model

MUTT is designed around a **horizontally scalable, shared-nothing microservices architecture**. This means that the primary method for increasing system capacity is to add more instances (pods/containers) of each service.

*   **Horizontal Scaling:** Each component (Ingestor, Alerter, Moog Forwarder, Web UI) is a stateless worker. You can increase the replica count of each service independently to meet performance demands.
*   **Stateless Workers:** Services do not store critical state locally. All state is externalized to Redis (for queues and transient state) or PostgreSQL (for long-term audit data). This allows new service instances to be added or removed without data loss or complex state migration.
*   **Centralized State:** Redis acts as the central nervous system, coordinating the work between the stateless services. The performance and availability of the Redis cluster are therefore critical to the overall system's scalability.

### When to Scale Each Component

*   **Scale Ingestors when:** HTTP request latency increases or CPU utilization on existing Ingestor instances is high.
*   **Scale Alerters when:** The `ingest_queue` depth in Redis is consistently growing. This indicates that events are being produced faster than they can be processed.
*   **Scale Moog Forwarders when:** The `alert_queue` depth is consistently growing. This means alerts are being generated by the Alerter faster than they can be sent to Moog.
*   **Scale Web UI when:** The API response time for management tasks becomes slow, or if a large number of operators are using the UI simultaneously.

---

## Capacity Planning

Capacity planning for MUTT involves sizing the infrastructure to meet the target Events Per Second (EPS).

### Events Per Second (EPS) Targets

The following are rough estimates. Actual performance will vary based on hardware, network latency, and rule complexity.

| Component         | Estimated EPS per Instance (per vCPU) | Primary Limiting Factor |
| ----------------- | ------------------------------------- | ----------------------- |
| **Ingestor**      | 1,000 - 2,000 EPS                     | CPU, Network I/O        |
| **Alerter**       | 500 - 1,000 EPS                       | DB Write Latency, CPU   |
| **Moog Forwarder**| ~50 EPS (by default)                  | Downstream Rate Limit   |

**Example Calculation:**
To achieve a target of **3,000 EPS**:
*   **Ingestors:** You would need 2-3 Ingestor instances (`3000 / 1500 = 2`).
*   **Alerters:** You would need 3-6 Alerter instances (`3000 / 750 = 4`).
*   **Moog Forwarder:** The number of forwarders does not increase the total throughput to Moog, which is governed by the shared rate limit (`MOOG_RATE_LIMIT`). You might scale it to 2-3 instances for high availability, not for performance.

### Redis Sizing Guidelines

*   **Memory:** Redis memory usage is primarily driven by queue depth. A rough estimate is `(average_message_size_in_bytes * max_expected_queue_depth) * 1.5`. For 1 million messages of 1KB each, you would need approximately 1.5 GB of memory for the queues alone.
*   **CPU:** Redis is typically single-threaded for command execution. A modern, high-frequency CPU core is more important than a large number of cores.
*   **High Availability:** For production, a **Redis Sentinel** setup with at least 3 nodes (1 primary, 2 replicas) is strongly recommended to provide automatic failover.

### PostgreSQL Sizing Guidelines

*   **Storage:** The primary driver of storage is the `event_audit_log`. The size can be estimated as `(avg_event_size * EPS * 3600 * 24 * retention_days)`. With monthly partitioning, you can manage this by archiving or dropping old partitions.
*   **I/O:** This is a write-heavy workload. Fast storage (SSDs) is critical for the performance of the Alerter service.
*   **Connections:** The number of allowed connections should be greater than the sum of the `maxconn` settings from the connection pools of all Alerter and Web UI instances.

---

## Performance Characteristics

### Latency Profiles

*   **Ingestor:** Very low latency (<10ms), as it only performs authentication, validation, and a single Redis `LPUSH`.
*   **Alerter:** Higher latency (10-100ms per event), as it involves database writes. The primary contributor to latency is the `INSERT` into the `event_audit_log`.
*   **Moog Forwarder:** Latency is dominated by the network round-trip to the Moog AIOps webhook API.

### Queue Depth Monitoring

The Redis queue depths are the most important indicators of system health and performance.

*   `mutt:ingest_queue`: Should ideally be close to zero. A consistently growing queue indicates that the **Alerter service is the bottleneck**.
*   `mutt:alert_queue`: Should also be close to zero. A growing queue indicates that the **Moog Forwarder is the bottleneck**, likely due to the downstream rate limit or an outage.
*   `mutt:dlq:*`: Any messages in a Dead Letter Queue indicate a problem with message format or a persistent failure that requires manual intervention.

---

## Scaling Strategies

### Ingestor Scaling

*   **Strategy:** Stateless, easy to scale.
*   **Action:** Increase the number of replicas behind a load balancer.
*   **Coordination:** No coordination is needed between instances.

### Alerter Scaling

*   **Strategy:** Horizontally scalable workers.
*   **Action:** Increase the number of Alerter replicas.
*   **Coordination:** The `BRPOPLPUSH` command on the shared `ingest_queue` acts as a natural load balancer. Each Alerter instance will pull messages from the queue as fast as it can process them. No other coordination is required.

### Moog Forwarder Scaling

*   **Strategy:** Scaled for high availability, not for performance.
*   **Action:** Run 2-3 replicas for redundancy.
*   **Coordination:** All instances coordinate via the Redis-based shared rate limiter (`mutt:rate_limit:moog`). Adding more forwarders **will not** increase the total number of requests sent to Moog; it will only distribute the sending of those requests across more instances.

### Database Scaling

*   **Strategy:** The database is the most difficult component to scale.
*   **Vertical Scaling:** The first step is to increase the resources of the PostgreSQL server (CPU, RAM, faster I/O).
*   **Read Replicas:** For read-heavy workloads on the Web UI, you can introduce a PostgreSQL read replica and point the Web UI service to it. This separates the read traffic from the critical write path of the Alerter.
*   **Partitioning:** The existing monthly partitioning of the `event_audit_log` is a critical scaling strategy that keeps the active table size manageable and makes data archival efficient.

---

## Bottleneck Identification

*   **Redis:** At very high EPS, the single-threaded nature of Redis can become a bottleneck. Monitor the CPU utilization of the Redis process. If it is consistently at 100%, you may need to scale vertically (faster CPU) or move to a Redis Cluster deployment.
*   **PostgreSQL Write Throughput:** This is the most likely bottleneck in the system. The Alerter's performance is directly tied to how quickly it can write to the `event_audit_log`. Monitor disk I/O and average query times.
*   **Moogsoft Rate Limits:** The throughput of the Moog Forwarder is almost always limited by the downstream API it is calling. The shared rate limiter is designed to manage this, but it means this part of the pipeline cannot be scaled indefinitely.
*   **Network Bandwidth:** In a very high-volume scenario, the network bandwidth between the services and the Redis/PostgreSQL servers can become a limiting factor.

---

## Optimization Strategies

*   **Alerter Rule Complexity:** The complexity of the regex rules in the Alerter can impact CPU usage. Prefer simple `contains` matches over complex `regex` matches where possible.
*   **Batch Processing:** For future optimization, services could be modified to process messages in batches rather than one by one. This would reduce the number of round-trips to Redis and PostgreSQL, but would increase code complexity.
*   **Connection Pooling Tuning:** The `minconn` and `maxconn` settings for the PostgreSQL connection pools in the Alerter and Web UI services can be tuned based on the number of replicas and expected load to optimize database performance.
*   **Redis Pipeline Usage:** The Ingestor service already uses Redis pipelines to batch multiple commands into a single round-trip. This pattern should be used in any new services that perform multiple consecutive Redis operations.
